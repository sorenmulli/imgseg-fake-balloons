\documentclass[12pt,fleqn]{article}

\usepackage[english]{babel}
\usepackage{SpeedyGonzales}
\usepackage{MediocreMike}
\usepackage{Blastoise}

\usepackage[backend=biber, style=alphabetic, citestyle=alphabetic, maxcitenames=4, maxbibnames=4, mincitenames=2]{biblatex}

\title{Synthesising Abstract Deep Computer Vision Pretraining Data}

\author{SÃ¸ren Winkel Holm, s183911}
\date{\today}

\pagestyle{fancy}
\fancyhf{}
\lhead{02561 Computer Graphics}
\chead{}
\rhead{Technical University of Denmark}
\lfoot{Final Project in Noise Functions}
\rfoot{Page \thepage{} of \pageref{LastPage}}

\graphicspath{{imgs/}}
\linespread{1.15}
\addbibresource{references.bib}

\begin{document}

\maketitle
\thispagestyle{fancy}
\tableofcontents
\newpage

%https://cocodataset.org/#detection-2016
%https://github.com/pytorch/vision/blob/main/references/segmentation/train.py

\section{Introduction}%
\label{sec:intro}
In the field of computer vision, deep learning is often used to solve problems such as object detection and image segmentation. Two main problems are (1) the high cost of annotating the image by producing the label maps (such as this) (2) the high cost of training the deep learning models. Automatic generation of scenes with corresponding label maps might be usable even if the scenes are abstract. Such data could be used to pretrain the deep learning model and act as an inductive bias such that less training and less data is required when actually training on the real dataset. For this to work, the generated scenes should have objects with object having some assigned class which determines some visual characteristics of the object. The scenes do not have to have anything to do with the actural task but need to make the model learn som basics of object detection, classification and segmentation. An easy first solution would be having the class determine the colour of the object but it is much more realistic if the objects are textured with noise functions and the parameters of the procedurally generated noise were determined by the class.

\section{Methods}%
\label{sec:method}
\subsection{Synthetic Scene Rendering}
\paragraph{Task 1: Noise Functions and The Simple Scene}
In the fragment shader, two noise functions were implemented, both accepting three-dimensional vector input.
\begin{enumerate}
    \item Improved Perlin noise (PN) using a permutation polynomial as \citeauthor{mcewan2012efficient} \cite{mcewan2012efficient}. 3D PN chooses pseudo-random gradients $\in \RR^3$ at regular grid points in each axis, interpolating a smooth function consistent with the gradients \cite{gustavson2005simplex}. 
    \item Sparse convolution noise (SC) as \citeauthor{frisvad2007fast} \cite{frisvad2007fast}. SC targets minimal artifacts or patterns by approximates the computationally expensive white noise filtered with a Gaussian kernel using a cubic kernel instead.
\end{enumerate}
Changing magnitude of the input vector -- the point at which the noise landscape is queried -- is equivalent to controlling the zoom level of the noise texture.
The implementations were largely based on reference implementations \cite{frisvad2016exploring}:
Only minor code changes and commenting were performed along with using the functions to texture the scene objects with a given noise scale.

An initial scene was constructed by rendering a coloured sphere through 5-fold recursive subdivision of tetrahedra.
The sphere was shaded using Phong shading from a positional light behind a perspective camera.
The sphere was textured using the noise functions evaluated at normals scaled with a free parameter called noise scale.
This texturing was perform by letting the noise intensity controlling the diffuse light intensity, see Listing \ref{lst:light}.

Behind the sphere, a 2D noise textured background in black and white was placed.

The scene and noise functions can be inspected with user control at \url{file:///home/sorenmulli/Nextcloud/cand2/computer-graphics/afl/webgl-site/t1.html}.
\lstinputlisting[language=C, firstline=134, lastline=146, caption=Fragment shader snippet related to texturing of foreground objects, label=lst:light]{../webgl-site/t1.html}

\paragraph{Task 2: Random Scenes}
Add multiple spheres to the scene, randomly selecting placement and size for each.
\paragraph{Task 3: Object Classes}
Choose a number of classes (maybe 3) and give each class a distribution of shapes, of colours and a distribution of noise parameters (such as related smoothing and scale). Assign each sphere to a class and let its shape, colour and noise be sampled from the class prior. (I will be using spheres and for shape I will use a model matrix that transforms the sphere into an ellipsoid by stretching it. The amount of stretching should thus be determined by object class)
\paragraph{Task 4: The Full Scene With Label Maps}
Add the possibility of producing the label image by having a rendering mode which does not have any lighting or texturing, just colours each object according to its class. This image will then have the class in the pixel position according to the object at that pixel position. Allow for saving the current canvas to a png.
\paragraph{Generated Datasets}


\subsection{Deep Image Segmentation}
\paragraph{Baseline Learning Task}
\paragraph{Addition of Synthetic Pretraining Data}

\section{Results}%
\label{sec:results}

\section{Discussion}%
\label{sec:disc}
\begin{enumerate}
    \item Powerful benchmark for different methods generalization towards different difficulties: Explainability
\end{enumerate}

\clearpage

\printbibliography[heading=bibintoc]


\end{document}
